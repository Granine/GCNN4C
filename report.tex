
\documentclass{article}
\usepackage[utf8]{inputenc}
% \usepackage[paperheight=16cm, paperwidth=12cm,% Set the height and width of the paper
% includehead,
% nomarginpar,% We don't want any margin paragraphs
% textwidth=10cm,% Set \textwidth to 10cm
% headheight=10mm,% Set \headheight to 10mm
% ]{geometry}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{bm}
\usepackage{xcolor}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{graphicx}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\iid{\emph{i.i.d}\onedot} \def\IID{\emph{I.I.D}\onedot}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\aka{\emph{a.k.a}\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\important}[1]{{\color{blue}{\bf\sf #1}}}

\title{CPEN455 Final Project: PixelCNN++G}
\author{Name: Guan Zheng Huang}
\date{Submit: 2024 April 21}


\begin{document}

\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[L]{\textbf{UBC CPEN455 2023 Winter Term 2}}
\fancyhead[R]{\textbf{Final Project: PixelCNN++G}}

\maketitle
\thispagestyle{fancy}
\section{Model}

\paragraph{Introduction}
PixelCNN++G is a conditional generative model based on the PixelCNN architecture. For this project, we aim to implement the PixelCNN++G model with an additional classification layer, making it an image classification model. The model will be trained on the CPEN450 dataset to classify images into one of four classes.

\paragraph{Model Description}
PixelCNN++G Innovative improvement

- DataPreprocessing
  - Since PixelCNN is a directional dependent model, meaning it have difficulty learning the orenentation of objects, we randomly flip the images horizontally during training to ensure the model is invariant to the direction of the image. 
  - Similarly, we perform rotation on images so the model can learn the orientation of the object, with reduce impact of the direction of the image.
- Conditional Model
    - The model is conditioned on the class label of the image, which is passed as an additional input to the model. This allows the model to learn class-specific features and improve classification accuracy.
    - A new set of weights is introduced at every layer of the upward and downward path (6 in total), each will be multiplied with the class label embedding before being used in the convolutional layers. This is an idea expanded on the [1] design of the Conditional PixelCNN, but insead of introducing bias to control the complete class shift, we introduce a set of weights to control the class shift at each layer, hoping that through adapting the label at each layer, a stronger correlation between the image and class can be established.
- Classification Layer
    - We utilized the loss function as the classification layer, with the idea that if we iterate through an image with all labels, the label that produces the lowest loss will be the label that the model predicts. This is a simple and effective way to perform classification without the need for additional layers or parameters.


\section{Experiments}
The model is trained on the CPEN450 dataset, 32x32 pixel images divided into four classes. With a batch size of 16 with 500 epochs, the additional conditional parameters are trained with xavier uniform initialization. 

\section{Attempted Solution}
Other attempted solution, which do not show as promising results, include:
- Directly embedding label information into the input or output of the model, which did not show significant improvement in classification accuracy.
- Using a PolyakAveragedModel to embed and concatenate with the input model at resnet block, which showed promissing results interms of FID of [] but did not show significant improvement in classification accuracy.
- A one-hot channel solution, with 4 additional channels to the input image, each representing a class, which did not show significant improvement in classification accuracy.
- Attempts to use a different downpass weight for every class have also been attempted, while this is proven to have a resonable image generation quality, it failed to obtain a strong FID score and have more randomized BPD values.

\paragraph{Results}
With the model trained, we achieved an accuracy of 44\% on the test set. The model is able to classify images into the correct class with reasonable accuracy.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.5\textwidth]{a1.png}
    \caption{Example image}
    \label{fig:exampleImage}
\end{figure}

\section{Conclusion}

\paragraph{Summary}
Provide a brief summary of the key findings from your experiments and the implications of these results. Recap the primary goal of the paper and whether it was achieved.


\section{Appendices or Supplemental Materials}

- **Source Code:** Include all relevant code used in your experiments, ideally with comments to aid understanding. Packaging this neatly in a zip file along with the report is often appreciated.
  
- **Experiment Hyper-Parameters:** Provide a detailed list or table of all hyper-parameters used, including those not discussed within the main text of the report.

- **Visualizations:** Include any additional graphical content that supports the conclusions drawn from your experiments. This might include generated images, detailed graphs, and charts that were too voluminous for the main sections.

This structure not only organizes your content logically but also guides the reader smoothly through your thought process, from introduction to conclusion. Remember, clarity and conciseness are key, so focus on creating a paper that delivers substantial information in a straightforward manner.

\end{document}